{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e426d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af09f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "X, y = fetch_20newsgroups_vectorized(subset='train', \n",
    "                                     return_X_y=True, \n",
    "                                     remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77232bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 114751), (11314,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47f55e",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2923d0",
   "metadata": {},
   "source": [
    "Dans la suite, on va garder les 5 classes les plus représentées parmi les 1000 premiers exemples de `X`. Les exemples associés autres autres classes sont écartés. On obtient finalement un échantillon que vous appelez `X_g` de 306 exemples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61a34c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 5\n",
    "n_exemples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b97b8",
   "metadata": {},
   "source": [
    "**Question**  Utiliser [np.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) avec l'argument `return_counts` pour identifier dans `labels` les classes que vous gardez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5caaaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X[:n_exemples]\n",
    "y = y[:n_exemples]\n",
    "# get most repeated classes\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "classes = classes[np.argsort(counts)][::-1][:nb_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0d516b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 13, 10,  6, 15])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d3f4f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306, 114751)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep only the most repeated classes\n",
    "mask = np.isin(y, classes)\n",
    "X_g = X[mask]\n",
    "\n",
    "y_g = y[mask]\n",
    "X_g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53d860",
   "metadata": {},
   "source": [
    "**Question** À l'aide de [np.in1d](https://numpy.org/doc/stable/reference/generated/numpy.in1d.htm), une ufunc qui teste l'appartenance à un ensemble, construire `X_g` et `y_g` les 306 exemples et leurs étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d81b74f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306, 114751)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with np.in1d\n",
    "mask = np.in1d(y, classes)\n",
    "X_gbis = X[mask]\n",
    "y_gbis = y[mask]\n",
    "X_gbis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04873ee7",
   "metadata": {},
   "source": [
    "Si vous n'avez pas réussi à faire cela, vous pouvez charger les données dans `pb.csv.xz`. (c'est un fichier compressé que panda sait lire)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4689f",
   "metadata": {},
   "source": [
    "# Encodage des classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26aef99",
   "metadata": {},
   "source": [
    "Dans la méthode de Zhou et al 2003, nous avons regardé l'identité entre un calcul itératif et une version analytique de ce calcul. Mais nous n'avons pas réellement défini une méthode de classification. En fait le problème de classification est résolu en prenant une fonction $F$ qui encode le problème de décision associé à la classification.\n",
    "\n",
    "La méthode s'applique généralement aux problèmes multiclasses, avec un nombre $c$ de classes. \n",
    "\n",
    "Pour chaque noeud, la fonction $F$ associe un score à chaque classe possible dans $[0,\\dots,c-1]$. Alors $F(i, k)$ est le score associé à la classe $k$ pour le noeud $i$. On fera une décision en prenant le score maximal associé à chaque noeud :\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathop{\\mathrm {argmax}}_{0\\leq k< c} F(i,k)\\hspace{5cm}(1)\n",
    "\\end{equation}\n",
    "\n",
    "En fait $F$ est l'encodage de `y_g` par un `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2c3ab",
   "metadata": {},
   "source": [
    "**Question** Calculer `F_star` qui est l'encodage des classes à l'aide de cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d9bf314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306, 5)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_g.reshape(-1,1)\n",
    "enc = OneHotEncoder()\n",
    "F_star = enc.fit_transform(y_g.reshape(-1,1)).toarray()\n",
    "F_star.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c0168",
   "metadata": {},
   "source": [
    "Dans ce problème semi-supervisé, la fonction $F$ est initialisée en mettant $F(i, k)=1$ pour tous les noeuds dont on connait l'étiquette associée $k$ et 0 partout ailleurs.  \n",
    "\n",
    "**Question** Définir `F`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a13a25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labeles =  300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4b081e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = F_star.copy()\n",
    "F[nb_labeles:, :] = 0\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01b407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c4ab6cf",
   "metadata": {},
   "source": [
    "# Label spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aab9920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e33951",
   "metadata": {},
   "source": [
    "**Question** Définir deux matrices `M_rbf` et `M_knn` pour calculer deux noyaux de similarité à partir \n",
    "- d'un noyau gaussien (on prendra $\\gamma=20$), et \n",
    "- d'un $k$ plus proches voisins (on prendra $k=3$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "29f19547",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_rbf = rbf_kernel(X_g, gamma=20)\n",
    "M_knn = kneighbors_graph(X_g, n_neighbors=3).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de41267",
   "metadata": {},
   "source": [
    "**Question** Définir une fonction `getS` qui prend en argument une matrice noyau et calcule la matrice $S$ associée à la méthode de Zhou et al 2003. NB: la diagonale de la matrice noyau doit être nulle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f808d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getS(M) : \n",
    "    M = M - np.diag(np.diag(M))\n",
    "    D = np.diag(M.sum(axis=1))\n",
    "    return D**(-1/2) @ M @ D**(-1/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844961bd",
   "metadata": {},
   "source": [
    "**Question** écrire une une fonction `zhou_iter` qui prend en argument, \n",
    "- une matrice noyau `M`, \n",
    "- une matrice `F` qui encode les classes (avec une partie supervisée et une partie non supervisée comme fait précédemment),\n",
    "- une liste de classes (comme `labels`) qui correspond à chaque colonne de `F`,\n",
    "- une valeur de $\\alpha$ (0.2 par défaut) et  \n",
    "- `max_iter` correspondant nombre maximal d'itérations. \n",
    "\n",
    "La fonction retourne le vecteur des classes correspondant au maximum définit par l'équation (1) ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "70680785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zhou_iter(M , f , labels , alpha = 0.2 , max_iter = 100) : \n",
    "    S = getS(M)\n",
    "    f_star = f.copy()\n",
    "    for _ in range(max_iter):\n",
    "        if np.allclose(f_star, f):\n",
    "            break\n",
    "        else : \n",
    "            f_star = alpha * S @ f_star + (1 - alpha) * labels\n",
    "        \n",
    "    return np.argmax(f_star , axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583e37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d782718",
   "metadata": {},
   "source": [
    "**Question** Écrire maintenant une fonction `zhou_analytique` qui fait ce calcul mais à l'aide de l'expression analytique. Les paramètres sont identiques (hormis `max_iter`). La valeur retournée est identique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da6be968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zhou_analytique(M, F, labels, alpha=0.2):\n",
    "    return labels[np.argmax(np.linalg.inv(np.eye(M.shape[0])- alpha*getS(M))@F, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef0039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0404798",
   "metadata": {},
   "source": [
    "**Question** Calculer l'erreur en classification avec vos implémentations de la méthode de Zhou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f01ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32439ffe",
   "metadata": {},
   "source": [
    "# Avec Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d500f",
   "metadata": {},
   "source": [
    "**Question** Refaire la même chose avec sklearn : calculer l'erreur de classification avec la méthode de Zhou implantée dans sklearn. (NB: le résultat peut être légèrement différent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e45ab30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "05ee3966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score :  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "label_prop_model = LabelPropagation(kernel='rbf', gamma=20, max_iter=1000)\n",
    "label_prop_model.fit(X_g[:nb_labeles], y_g[:nb_labeles])\n",
    "y_pred = label_prop_model.predict(X_g[nb_labeles:])\n",
    "print(\"accuracy score : \", accuracy_score(y_g[nb_labeles:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa219d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633b6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48055b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf2523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
